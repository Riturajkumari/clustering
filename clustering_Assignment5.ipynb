{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB4trMT7OFXlop0ypeLNrm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/clustering/blob/main/clustering_Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
      ],
      "metadata": {
        "id": "Gs5WtGQBgicg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A contingency matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It is also known as a confusion matrix.\n",
        "- The matrix compares the predicted values from the model with the actual values. The matrix has four cells, which represent the number of true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN). The TP and TN cells represent correct predictions, while the FP and FN cells represent incorrect predictions. The contingency matrix is used to calculate various performance metrics such as accuracy, precision, recall, F1 score."
      ],
      "metadata": {
        "id": "dnm_-kwWivY-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Tl6SycVgls0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
        "certain situations?"
      ],
      "metadata": {
        "id": "po3Z9zhsgl58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A pair confusion matrix is a similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings.\n",
        "- It computes a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings."
      ],
      "metadata": {
        "id": "_NGMH40JjTI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A regular confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It is a table with four different combinations of predicted and actual values."
      ],
      "metadata": {
        "id": "xnPb4w_xjksh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8e4EMAlbgp_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
        "used to evaluate the performance of language models?"
      ],
      "metadata": {
        "id": "jQQ2tRj3gqTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In natural language processing (NLP), extrinsic evaluation considers the NLP system in a more complex setting, either as an embedded system or serving a precise function for a human user. Extrinsic evaluators use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. Extrinsic metrics have been introduced to measure fairness in downstream applications.\n",
        "- Intrinsic evaluation tests the quality of a representation independent of specific natural language processing tasks while extrinsic evaluators use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task."
      ],
      "metadata": {
        "id": "FdYWebmwj307"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "909xtukqiE8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
        "extrinsic measure?"
      ],
      "metadata": {
        "id": "RICLwBLfiFXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Intrinsic measures are the measures that just take the generated forecast and ground truth to compute the metric. Extrinsic measures are measures that use an external reference forecast also in addition to the generated forecast and ground truth to compute the metric.\n",
        "- Intrinsic evaluation is the evaluation of a set of word vectors generated by an embedding technique (such as Word2Vec or GloVe) on specific intermediate subtasks (such as analogy completion). These subtasks are typically simple and fast to compute and thereby allow us to help understand the system used to generate the word vectors.\n",
        "- An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask."
      ],
      "metadata": {
        "id": "u1BZdi5rkeQq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsF8QZAUiJmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
        "strengths and weaknesses of a model?"
      ],
      "metadata": {
        "id": "RUfnFK6hiJw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A confusion matrix is a matrix or a table that shows the performance of a classification model on a set of test data with known true values. It is used to measure accuracy, recall, precision, specificity, and other metrics for classification models with two or more classes. It is also known as an error matrix.\n",
        "- The confusion matrix is used to identify the strengths and weaknesses of a model. It provides a summary of the number of correct and incorrect predictions made by the classifier. The diagonal elements represent the number of correct predictions, while the off-diagonal elements represent the number of incorrect predictions.\n",
        "- For example, consider a binary classification problem where we have two classes: positive and negative. The confusion matrix for this problem would be a 2x2 matrix with four entries: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). TP represents the number of positive instances that are correctly classified as positive. FP represents the number of negative instances that are incorrectly classified as positive. TN represents the number of negative instances that are correctly classified as negative. FN represents the number of positive instances that are incorrectly classified as negative."
      ],
      "metadata": {
        "id": "2b_RKQoLk8QI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEjWvRogiNYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwH79xXAiON4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
        "learning algorithms, and how can they be interpreted?"
      ],
      "metadata": {
        "id": "0T7iQ901iOsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several intrinsic measures used to evaluate the performance of unsupervised learning algorithms. Some of the common intrinsic measures are:\n",
        "\n",
        "- Silhouette Coefficient: This is a measure of how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where a higher value indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
        "- Davies-Bouldin Index: This index measures the average similarity between each cluster and its most similar cluster. A lower value indicates better clustering.\n",
        "- Calinski-Harabasz Index: This index measures the ratio of between-cluster variance to within-cluster variance. A higher value indicates better clustering.\n",
        "- Dunn Index: This index measures the ratio of the minimum distance between points in different clusters to the maximum diameter of any cluster. A higher value indicates better clustering."
      ],
      "metadata": {
        "id": "Q5r2YVxPlcQD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmHnyq5-iWlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
        "how can these limitations be addressed?"
      ],
      "metadata": {
        "id": "8ODeoZONiXXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy is a commonly used metric for classification tasks. However, it has some limitations. For example, it does not take into account the cost of false positives and false negatives. In some cases, a false positive may be more costly than a false negative or vice versa. In such cases, other metrics such as precision and recall may be more appropriate.\n",
        "- Another limitation of accuracy is that it does not account for class imbalance. When the classes are imbalanced, accuracy can be misleading. For example, if one class has 90% of the data and the other class has 10%, a classifier that always predicts the majority class will have an accuracy of 90%. However, this classifier is not useful because it does not predict the minority class at all.\n",
        "- To address these limitations, other metrics such as precision, recall, F1 score, and area under the ROC curve can be used2. These metrics provide a more nuanced view of the performance of a classifier and can help identify areas for improvement."
      ],
      "metadata": {
        "id": "C4QTjzHjl0Ho"
      }
    }
  ]
}